{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3. 토크나이저 불러오기"
      ],
      "metadata": {
        "id": "V9HV0EiTV4SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 업로드 (Google Colab의 경우)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 파일 불러오기\n",
        "with open('X_train_tokenized.pkl', 'rb') as f:\n",
        "    X_train_tokenized = pickle.load(f)\n",
        "\n",
        "with open('X_test_tokenized.pkl', 'rb') as f:\n",
        "    X_test_tokenized = pickle.load(f)\n",
        "\n",
        "with open('tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "bVfQVUPAV23H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Attention + LSTM 모델\n",
        "\n",
        "Attention 매커니즘"
      ],
      "metadata": {
        "id": "9_Q62ntdVlVz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1GPAtwGVd_B"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Attention 가중치를 위한 weight 생성\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Attention score 계산\n",
        "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
        "        a = tf.keras.backend.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return tf.keras.backend.sum(output, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM 구현"
      ],
      "metadata": {
        "id": "XC_jqhRNZtS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
        "\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1  # 단어 사전의 크기\n",
        "EMBEDDING_DIM = 128  # 임베딩 벡터의 차원\n",
        "MAX_LENGTH = 100  # 입력 시퀀스의 최대 길이 (앞서 패딩 처리할 때 설정한 값)\n",
        "\n",
        "# 모델 구성\n",
        "input_layer = Input(shape=(MAX_LENGTH,))\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMBEDDING_DIM)(input_layer)\n",
        "lstm_layer = LSTM(128, return_sequences=True)(embedding_layer)\n",
        "attention_layer = AttentionLayer()(lstm_layer)\n",
        "output_layer = Dense(1, activation=\"sigmoid\")(attention_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "w5ph8VveVphB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 KoBERT 모델\n",
        "\n",
        "KoBERT를 활용하려면 Huggingface의 Transformers 라이브러리가 필요합니다. KoBERT 모델을 불러와서 사용할 수 있습니다."
      ],
      "metadata": {
        "id": "m5Qg33nPYFyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# KoBERT 토큰화기와 모델 불러오기\n",
        "tokenizer_kobert = BertTokenizer.from_pretrained('monologg/kobert')\n",
        "model_kobert = TFBertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2)\n",
        "\n",
        "# 모델 구성 및 컴파일\n",
        "model_kobert.compile(optimizer=Adam(learning_rate=5e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_kobert.summary()"
      ],
      "metadata": {
        "id": "Wb-zSnqaYEo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 두 모델이 준비되었습니다. 각각의 모델로 텍스트 데이터를 학습시키면 스팸 여부를 분류할 수 있게 됩니다."
      ],
      "metadata": {
        "id": "QEup-UwIZxrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 학습 및 검증\n",
        "\n",
        "#### 3.1 Attention + LSTM 모델\n",
        "\n",
        "1. **모델에 Dropout 추가**:\n",
        "Dropout을 LSTM 레이어와 출력 레이어 사이에 추가합니다."
      ],
      "metadata": {
        "id": "vtikCI6ZZM1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "input_layer = Input(shape=(MAX_LENGTH,))\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMBEDDING_DIM)(input_layer)\n",
        "lstm_layer = LSTM(128, return_sequences=True)(embedding_layer)\n",
        "attention_layer = AttentionLayer()(lstm_layer)\n",
        "dropout_layer = Dropout(0.5)(attention_layer)  # Dropout 추가\n",
        "output_layer = Dense(1, activation=\"sigmoid\")(dropout_layer)\n",
        "\n",
        "model_attention_lstm = Model(inputs=input_layer, outputs=output_layer)\n",
        "model_attention_lstm.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "bVaq53X7ZNon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Early Stopping 추가 및 모델 학습**:"
      ],
      "metadata": {
        "id": "eL9dZa6bbRYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history_attention_lstm = model_attention_lstm.fit(\n",
        "    X_train_padded, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_padded, y_test),\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "ii_FLJuEbRJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 KoBERT 모델\n",
        "\n",
        "1. **Early Stopping 추가 및 모델 학습**:\n",
        "\n",
        "KoBERT를 사용할 때, 입력 데이터는 토큰화 후 패딩 처리된 것이 아닌 원래의 텍스트 데이터를 사용해야 합니다.\n"
      ],
      "metadata": {
        "id": "mhE_m4WPZ5aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer_kobert(list(X_train), truncation=True, padding=True, max_length=MAX_LENGTH)\n",
        "test_encodings = tokenizer_kobert(list(X_test), truncation=True, padding=True, max_length=MAX_LENGTH)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history_kobert = model_kobert.fit(\n",
        "    train_encodings, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(test_encodings, y_test),\n",
        "    callbacks=[early_stopping]"
      ],
      "metadata": {
        "id": "tw2ruJwzZ5PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 코드를 통해 각 모델을 학습하고 검증 데이터를 사용하여 성능을 평가할 수 있습니다. Early Stopping을 사용하여 성능 향상이 없을 때 학습을 조기 종료하는 기법을 적용하였습니다."
      ],
      "metadata": {
        "id": "0VGMWYnmZ_uB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 앙상블\n",
        "\n",
        "앙상블에서 가장 간단하면서도 효과적인 방법은 소프트 투표(확률 평균)입니다. 각 모델의 예측 확률을 평균내어 최종 예측 확률을 계산하고, 그 확률을 기반으로 분류를 수행합니다.\n"
      ],
      "metadata": {
        "id": "EgONcodDn2zY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 모델의 예측 확률 계산\n",
        "prob_attention_lstm = model_attention_lstm.predict(X_test_padded)\n",
        "prob_kobert = model_kobert.predict(test_encodings)[0]  # transformers 모델의 출력은 (logit, ...) 형태이므로 [0] 선택\n",
        "\n",
        "# 확률을 평균내어 최종 예측 확률 계산\n",
        "final_prob = (prob_attention_lstm + prob_kobert) / 2\n",
        "\n",
        "# 확률을 기반으로 최종 분류 수행\n",
        "final_predictions = [1 if prob > 0.5 else 0 for prob in final_prob]"
      ],
      "metadata": {
        "id": "Nu8TaqUNZ_hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 최종 평가\n",
        "\n",
        "최종적으로 앙상블 모델의 성능을 평가합니다. 분류 문제에서 일반적으로 사용되는 지표는 정확도, 정밀도, 재현율, F1-점수 등입니다.\n"
      ],
      "metadata": {
        "id": "XtR9wtstn-cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"정확도:\", accuracy_score(y_test, final_predictions))\n",
        "print(\"정밀도:\", precision_score(y_test, final_predictions))\n",
        "print(\"재현율:\", recall_score(y_test, final_predictions))\n",
        "print(\"F1-점수:\", f1_score(y_test, final_predictions))"
      ],
      "metadata": {
        "id": "mz660PsDn-FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "위의 코드를 통해 앙상블 모델을 구성하고 최종 성능을 평가할 수 있습니다."
      ],
      "metadata": {
        "id": "gj7-MrcHoC-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 하이퍼파라미터 튜닝\n",
        "\n",
        "하이퍼파라미터 튜닝은 모델의 성능을 향상시키는 중요한 단계입니다."
      ],
      "metadata": {
        "id": "IVefDeegojNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention + LSTM 모델:\n",
        "\n",
        "1. **임베딩 차원(EMBEDDING_DIM)**: 단어를 몇 차원의 벡터로 표현할지 결정합니다.\n",
        "2. **LSTM 유닛의 수**: LSTM 레이어의 유닛 수를 조정합니다.\n",
        "3. **Dropout 비율**: 오버피팅을 방지하기 위한 Dropout의 비율을 조정합니다.\n",
        "4. **학습률(learning rate)**: 옵티마이저의 학습률을 조정합니다.\n",
        "5. **배치 크기(batch size)**: 한 번에 학습하는 데이터의 수를 조정합니다.\n",
        "\n",
        "#### KoBERT 모델:\n",
        "\n",
        "1. **학습률(learning rate)**: 옵티마이저의 학습률을 조정합니다.\n",
        "2. **배치 크기(batch size)**: 한 번에 학습하는 데이터의 수를 조정합니다.\n",
        "\n",
        "### 2. 튜닝 방법:\n",
        "\n",
        "#### Grid Search:\n",
        "- 가능한 모든 조합의 하이퍼파라미터 값을 시도하여 최적의 값을 찾습니다.\n",
        "- 시간이 오래 걸리는 단점이 있습니다.\n",
        "\n",
        "#### Random Search:\n",
        "- 주어진 하이퍼파라미터 값의 범위 내에서 무작위로 값을 선택하여 최적의 값을 찾습니다.\n",
        "- Grid Search보다 빠르게 근사치를 찾을 수 있습니다.\n",
        "\n",
        "### 조언:\n",
        "\n",
        "1. **시간과 리소스**: Grid Search는 모든 조합을 시도하기 때문에 많은 시간과 리소스가 필요합니다. 이에 반해 Random Search는 주어진 시간 내에서 더 다양한 조합을 탐색할 수 있습니다. 따라서, 초기 단계에서는 Random Search를 사용하여 대략적인 최적의 하이퍼파라미터 범위를 찾은 후, 그 범위 내에서 Grid Search를 수행하는 것이 좋습니다.\n",
        "\n",
        "2. **하이퍼파라미터 범위 설정**: 주요 튜닝 대상 하이퍼파라미터에 대해 넓은 범위를 설정하여 탐색을 시작하고, 점차 범위를 좁혀 나가는 것이 효과적입니다.\n",
        "\n",
        "3. **Early Stopping**: 하이퍼파라미터 튜닝 과정에서도 Early Stopping을 활용하여 학습 시간을 단축시킬 수 있습니다.\n",
        "\n",
        "4. **교차 검증(Cross Validation)**: 튜닝 과정에서 교차 검증을 활용하면 모델의 일반화 성능을 더 정확하게 평가할 수 있습니다. 하지만 시간이 더 많이 소요됩니다.\n",
        "\n",
        "최적의 하이퍼파라미터를 찾는 과정은 많은 시간과 실험이 필요합니다. 따라서, 충분한 시간과 리소스를 확보하고 여러 번의 시도를 통해 최적의 값을 찾는 것이 중요합니다."
      ],
      "metadata": {
        "id": "PKGEi8H6ovGL"
      }
    }
  ]
}