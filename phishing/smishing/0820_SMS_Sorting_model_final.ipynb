{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## [지정 경로에 모델을 배치하고 실행하는 경우]\n",
        "\n",
        "\n",
        "```\n",
        "# 다음 경로에 모델 파일을 배치하고 실행 요망.\n",
        "('/content/drive/MyDrive/finalmodel/model_kobert')\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "62XK-x_9EP6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 불러오기"
      ],
      "metadata": {
        "id": "xIMskjMtE5Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# 저장한 모델 불러오기\n",
        "model_attention_lstm = load_model(save_path_attention_lstm, custom_objects={\"AttentionLayer\": AttentionLayer})\n",
        "\n",
        "# 모델 요약 출력\n",
        "model_attention_lstm.summary()\n",
        "\n",
        "print(f\"모델이 '{save_path_attention_lstm}' 경로에서 불러와졌습니다.\")"
      ],
      "metadata": {
        "id": "9_Yk9_l6E4Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "\n",
        "# 저장한 모델과 토크나이저 불러오기\n",
        "loaded_tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/finalmodel/model_kobert')\n",
        "loaded_model = TFBertForSequenceClassification.from_pretrained('/content/drive/MyDrive/finalmodel/model_kobert')\n",
        "\n",
        "print(\"모델과 토크나이저가 불러와졌습니다.\")"
      ],
      "metadata": {
        "id": "nYCz4FcrEYni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 앙상블"
      ],
      "metadata": {
        "id": "KYvNrV48FQ7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "# 모델 컴파일\n",
        "loaded_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 테스트 데이터의 인코딩\n",
        "test_encodings = loaded_tokenizer(list(X_test), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n",
        "\n",
        "# 각 모델의 예측 결과 계산\n",
        "prob_kobert = loaded_model.predict([test_encodings['input_ids'], test_encodings['attention_mask']]).logits\n",
        "prob_attention_lstm = model_attention_lstm.predict(X_test_padded)\n",
        "\n",
        "# 로짓 값을 확률로 변환\n",
        "probabilities_kobert = tf.nn.sigmoid(prob_kobert)\n",
        "probabilities_attention_lstm = tf.nn.sigmoid(prob_attention_lstm)\n",
        "\n",
        "# 앙상블을 위한 예측 확률 계산\n",
        "ensemble_probabilities = (probabilities_kobert + probabilities_attention_lstm) / 2\n",
        "\n",
        "# 앙상블 결과를 소프트 보팅하여 예측 클래스 계산\n",
        "ensemble_predicted_classes = np.argmax(ensemble_probabilities, axis=1)\n",
        "\n",
        "# 정확도 평가\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_predicted_classes)\n",
        "print(f\"모델 앙상블의 예측 정확도: {ensemble_accuracy:.4f}\")\n",
        "\n",
        "# 로스 평가\n",
        "ensemble_loss = log_loss(y_test, ensemble_probabilities)\n",
        "print(f\"모델 앙상블의 로스: {ensemble_loss:.4f}\")"
      ],
      "metadata": {
        "id": "BaGpdI-3FQT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 테스트"
      ],
      "metadata": {
        "id": "SyQk1TfgFXZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 불러온 모델 및 토크나이저 설정\n",
        "loaded_tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/finalmodel/model_kobert')\n",
        "loaded_model = TFBertForSequenceClassification.from_pretrained('/content/drive/MyDrive/finalmodel/model_kobert')\n",
        "\n",
        "# 텍스트 입력 받기\n",
        "text = input(\"예측할 텍스트를 입력하세요: \")\n",
        "\n",
        "# 입력 텍스트의 전처리 및 인코딩\n",
        "encoded_input = loaded_tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "\n",
        "# 각 모델의 예측 결과 계산\n",
        "prob_kobert = loaded_model.predict([encoded_input['input_ids'], encoded_input['attention_mask']]).logits\n",
        "prob_attention_lstm = model_attention_lstm.predict(X_test_padded)\n",
        "\n",
        "# 로짓 값을 확률로 변환\n",
        "probabilities_kobert = tf.nn.sigmoid(prob_kobert)\n",
        "probabilities_attention_lstm = tf.nn.sigmoid(prob_attention_lstm)\n",
        "\n",
        "# 앙상블을 위한 예측 확률 계산\n",
        "ensemble_probabilities = (probabilities_kobert + probabilities_attention_lstm) / 2\n",
        "\n",
        "# 앙상블 결과를 소프트 보팅하여 예측 클래스 계산\n",
        "ensemble_predicted_class = np.argmax(ensemble_probabilities, axis=1)[0]\n",
        "\n",
        "# 예측 클래스와 확률 출력\n",
        "class_names = ['정상', '스팸']  # 클래스 이름 설정\n",
        "predicted_class_name = class_names[ensemble_predicted_class]\n",
        "predicted_class_probability = ensemble_probabilities[0][ensemble_predicted_class]\n",
        "print(f\"예측: {predicted_class_name}\")\n",
        "print(f\"확률: {predicted_class_probability:.4f}\")"
      ],
      "metadata": {
        "id": "oVTUZsHPFaLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [모델 없이 데이터셋만 있는 경우]\n",
        "(이 경우는 데이터셋 CSV 파일은 있어야 합니다.)"
      ],
      "metadata": {
        "id": "j2zNMG-ZEjHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. 학습 준비"
      ],
      "metadata": {
        "id": "Apj3r_VOFxqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1. 모듈 및 데이터셋 불러오기"
      ],
      "metadata": {
        "id": "SHbuE0NAGgzm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ah3tzjOMLlK2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4ffmd0WZ7vNL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26adc078-114c-43c1-d1d3-9c72b80c81b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ND-nuPx_MU9e"
      },
      "outputs": [],
      "source": [
        "# df_97이라는 데이터가 있다고 가정\n",
        "df_97 = pd.read_csv(\"/content/drive/MyDrive/spam_data_free/combine_spam_labeled.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_97.shape"
      ],
      "metadata": {
        "id": "pCPud_MKvzm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd0c46ea-9ad1-4acd-b7fa-2a7983adef51"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(494615, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2. 샘플링(샘플 데이터 학습 시에만 실행)"
      ],
      "metadata": {
        "id": "z4fexEmzGGmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 라벨 0과 라벨 1 데이터를 분리합니다.\n",
        "df_97_label0 = df_97[df_97['label'] == 0]\n",
        "df_97_label1 = df_97[df_97['label'] == 1]\n",
        "\n",
        "# 각 라벨별로 10만 개씩 무작위로 샘플링합니다.\n",
        "sampled_label0 = df_97_label0.sample(n=20000, random_state=42)\n",
        "sampled_label1 = df_97_label1.sample(n=20000, random_state=42)\n",
        "\n",
        "# 샘플링된 데이터를 합쳐서 새로운 데이터셋을 생성합니다.\n",
        "df_99 = pd.concat([sampled_label0, sampled_label1], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(df_99['label'].value_counts())\n"
      ],
      "metadata": {
        "id": "VkgEVlK0Dt-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cfe65a4-df94-424b-ffbd-7d024106797a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    20000\n",
            "0    20000\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_99.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTeqQEk8F7VO",
        "outputId": "b7852322-6e65-4c01-f797-0bb27627b0f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_97 = df_99"
      ],
      "metadata": {
        "id": "cTi1m66gGNOJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3. test/train 데이터 분리"
      ],
      "metadata": {
        "id": "0YpWIqsEGTAC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QOHg6i0xLiKf"
      },
      "outputs": [],
      "source": [
        "# 데이터를 학습 데이터와 테스트 데이터로 나눕니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_97['utterances'], df_97['label'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4. 토큰화\n"
      ],
      "metadata": {
        "id": "ktxQUHc4Ganw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "53xSkbgPQ-Dr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3cb252c-bf10-4fe8-f4d1-73379e8214c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# Okt 객체 생성\n",
        "okt = Okt()\n",
        "\n",
        "def tokenize(texts, saved_filename):\n",
        "    \"\"\"주어진 텍스트 리스트를 토큰화하고 진행 상황을 출력합니다.\"\"\"\n",
        "    tokenized_texts = []\n",
        "\n",
        "    # 이전에 저장된 토큰화 결과가 있다면 불러옵니다.\n",
        "    if os.path.exists(saved_filename):\n",
        "        with open(saved_filename, 'r', encoding='utf-8') as f:\n",
        "            tokenized_texts = [line.strip() for line in f]\n",
        "\n",
        "    start_idx = len(tokenized_texts)\n",
        "    for idx, text in enumerate(texts[start_idx:]):\n",
        "        if (idx + 1) % 200 == 0:\n",
        "            print(f\"{idx + 1 + start_idx}개의 텍스트를 토큰화했습니다.\")\n",
        "            # 중간 결과를 저장합니다.\n",
        "            with open(saved_filename, 'a', encoding='utf-8') as f:\n",
        "                for tokens in tokenized_texts[idx-199:idx+1]:\n",
        "                    f.write(' '.join(tokens) + '\\n')\n",
        "\n",
        "        tokenized_texts.append(okt.morphs(text))\n",
        "\n",
        "    return tokenized_texts\n",
        "\n",
        "# 학습 데이터와 테스트 데이터를 토큰화합니다.\n",
        "X_train_tokenized = tokenize(X_train, 'X_train_tokenized.txt')\n",
        "X_test_tokenized = tokenize(X_test, 'X_test_tokenized.txt')\n",
        "\n",
        "print(\"토큰화 완료!\")\n"
      ],
      "metadata": {
        "id": "AfT7RlFDFHdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "845c5d31-94d7-4df9-f56c-eda03173b67b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200개의 텍스트를 토큰화했습니다.\n",
            "400개의 텍스트를 토큰화했습니다.\n",
            "600개의 텍스트를 토큰화했습니다.\n",
            "800개의 텍스트를 토큰화했습니다.\n",
            "1000개의 텍스트를 토큰화했습니다.\n",
            "1200개의 텍스트를 토큰화했습니다.\n",
            "1400개의 텍스트를 토큰화했습니다.\n",
            "1600개의 텍스트를 토큰화했습니다.\n",
            "1800개의 텍스트를 토큰화했습니다.\n",
            "2000개의 텍스트를 토큰화했습니다.\n",
            "2200개의 텍스트를 토큰화했습니다.\n",
            "2400개의 텍스트를 토큰화했습니다.\n",
            "2600개의 텍스트를 토큰화했습니다.\n",
            "2800개의 텍스트를 토큰화했습니다.\n",
            "3000개의 텍스트를 토큰화했습니다.\n",
            "3200개의 텍스트를 토큰화했습니다.\n",
            "3400개의 텍스트를 토큰화했습니다.\n",
            "3600개의 텍스트를 토큰화했습니다.\n",
            "3800개의 텍스트를 토큰화했습니다.\n",
            "4000개의 텍스트를 토큰화했습니다.\n",
            "4200개의 텍스트를 토큰화했습니다.\n",
            "4400개의 텍스트를 토큰화했습니다.\n",
            "4600개의 텍스트를 토큰화했습니다.\n",
            "4800개의 텍스트를 토큰화했습니다.\n",
            "5000개의 텍스트를 토큰화했습니다.\n",
            "5200개의 텍스트를 토큰화했습니다.\n",
            "5400개의 텍스트를 토큰화했습니다.\n",
            "5600개의 텍스트를 토큰화했습니다.\n",
            "5800개의 텍스트를 토큰화했습니다.\n",
            "6000개의 텍스트를 토큰화했습니다.\n",
            "6200개의 텍스트를 토큰화했습니다.\n",
            "6400개의 텍스트를 토큰화했습니다.\n",
            "6600개의 텍스트를 토큰화했습니다.\n",
            "6800개의 텍스트를 토큰화했습니다.\n",
            "7000개의 텍스트를 토큰화했습니다.\n",
            "7200개의 텍스트를 토큰화했습니다.\n",
            "7400개의 텍스트를 토큰화했습니다.\n",
            "7600개의 텍스트를 토큰화했습니다.\n",
            "7800개의 텍스트를 토큰화했습니다.\n",
            "8000개의 텍스트를 토큰화했습니다.\n",
            "8200개의 텍스트를 토큰화했습니다.\n",
            "8400개의 텍스트를 토큰화했습니다.\n",
            "8600개의 텍스트를 토큰화했습니다.\n",
            "8800개의 텍스트를 토큰화했습니다.\n",
            "9000개의 텍스트를 토큰화했습니다.\n",
            "9200개의 텍스트를 토큰화했습니다.\n",
            "9400개의 텍스트를 토큰화했습니다.\n",
            "9600개의 텍스트를 토큰화했습니다.\n",
            "9800개의 텍스트를 토큰화했습니다.\n",
            "10000개의 텍스트를 토큰화했습니다.\n",
            "10200개의 텍스트를 토큰화했습니다.\n",
            "10400개의 텍스트를 토큰화했습니다.\n",
            "10600개의 텍스트를 토큰화했습니다.\n",
            "10800개의 텍스트를 토큰화했습니다.\n",
            "11000개의 텍스트를 토큰화했습니다.\n",
            "11200개의 텍스트를 토큰화했습니다.\n",
            "11400개의 텍스트를 토큰화했습니다.\n",
            "11600개의 텍스트를 토큰화했습니다.\n",
            "11800개의 텍스트를 토큰화했습니다.\n",
            "12000개의 텍스트를 토큰화했습니다.\n",
            "12200개의 텍스트를 토큰화했습니다.\n",
            "12400개의 텍스트를 토큰화했습니다.\n",
            "12600개의 텍스트를 토큰화했습니다.\n",
            "12800개의 텍스트를 토큰화했습니다.\n",
            "13000개의 텍스트를 토큰화했습니다.\n",
            "13200개의 텍스트를 토큰화했습니다.\n",
            "13400개의 텍스트를 토큰화했습니다.\n",
            "13600개의 텍스트를 토큰화했습니다.\n",
            "13800개의 텍스트를 토큰화했습니다.\n",
            "14000개의 텍스트를 토큰화했습니다.\n",
            "14200개의 텍스트를 토큰화했습니다.\n",
            "14400개의 텍스트를 토큰화했습니다.\n",
            "14600개의 텍스트를 토큰화했습니다.\n",
            "14800개의 텍스트를 토큰화했습니다.\n",
            "15000개의 텍스트를 토큰화했습니다.\n",
            "15200개의 텍스트를 토큰화했습니다.\n",
            "15400개의 텍스트를 토큰화했습니다.\n",
            "15600개의 텍스트를 토큰화했습니다.\n",
            "15800개의 텍스트를 토큰화했습니다.\n",
            "16000개의 텍스트를 토큰화했습니다.\n",
            "16200개의 텍스트를 토큰화했습니다.\n",
            "16400개의 텍스트를 토큰화했습니다.\n",
            "16600개의 텍스트를 토큰화했습니다.\n",
            "16800개의 텍스트를 토큰화했습니다.\n",
            "17000개의 텍스트를 토큰화했습니다.\n",
            "17200개의 텍스트를 토큰화했습니다.\n",
            "17400개의 텍스트를 토큰화했습니다.\n",
            "17600개의 텍스트를 토큰화했습니다.\n",
            "17800개의 텍스트를 토큰화했습니다.\n",
            "18000개의 텍스트를 토큰화했습니다.\n",
            "18200개의 텍스트를 토큰화했습니다.\n",
            "18400개의 텍스트를 토큰화했습니다.\n",
            "18600개의 텍스트를 토큰화했습니다.\n",
            "18800개의 텍스트를 토큰화했습니다.\n",
            "19000개의 텍스트를 토큰화했습니다.\n",
            "19200개의 텍스트를 토큰화했습니다.\n",
            "19400개의 텍스트를 토큰화했습니다.\n",
            "19600개의 텍스트를 토큰화했습니다.\n",
            "19800개의 텍스트를 토큰화했습니다.\n",
            "20000개의 텍스트를 토큰화했습니다.\n",
            "20200개의 텍스트를 토큰화했습니다.\n",
            "20400개의 텍스트를 토큰화했습니다.\n",
            "20600개의 텍스트를 토큰화했습니다.\n",
            "20800개의 텍스트를 토큰화했습니다.\n",
            "21000개의 텍스트를 토큰화했습니다.\n",
            "21200개의 텍스트를 토큰화했습니다.\n",
            "21400개의 텍스트를 토큰화했습니다.\n",
            "21600개의 텍스트를 토큰화했습니다.\n",
            "21800개의 텍스트를 토큰화했습니다.\n",
            "22000개의 텍스트를 토큰화했습니다.\n",
            "22200개의 텍스트를 토큰화했습니다.\n",
            "22400개의 텍스트를 토큰화했습니다.\n",
            "22600개의 텍스트를 토큰화했습니다.\n",
            "22800개의 텍스트를 토큰화했습니다.\n",
            "23000개의 텍스트를 토큰화했습니다.\n",
            "23200개의 텍스트를 토큰화했습니다.\n",
            "23400개의 텍스트를 토큰화했습니다.\n",
            "23600개의 텍스트를 토큰화했습니다.\n",
            "23800개의 텍스트를 토큰화했습니다.\n",
            "24000개의 텍스트를 토큰화했습니다.\n",
            "24200개의 텍스트를 토큰화했습니다.\n",
            "24400개의 텍스트를 토큰화했습니다.\n",
            "24600개의 텍스트를 토큰화했습니다.\n",
            "24800개의 텍스트를 토큰화했습니다.\n",
            "25000개의 텍스트를 토큰화했습니다.\n",
            "25200개의 텍스트를 토큰화했습니다.\n",
            "25400개의 텍스트를 토큰화했습니다.\n",
            "25600개의 텍스트를 토큰화했습니다.\n",
            "25800개의 텍스트를 토큰화했습니다.\n",
            "26000개의 텍스트를 토큰화했습니다.\n",
            "26200개의 텍스트를 토큰화했습니다.\n",
            "26400개의 텍스트를 토큰화했습니다.\n",
            "26600개의 텍스트를 토큰화했습니다.\n",
            "26800개의 텍스트를 토큰화했습니다.\n",
            "27000개의 텍스트를 토큰화했습니다.\n",
            "27200개의 텍스트를 토큰화했습니다.\n",
            "27400개의 텍스트를 토큰화했습니다.\n",
            "27600개의 텍스트를 토큰화했습니다.\n",
            "27800개의 텍스트를 토큰화했습니다.\n",
            "28000개의 텍스트를 토큰화했습니다.\n",
            "28200개의 텍스트를 토큰화했습니다.\n",
            "28400개의 텍스트를 토큰화했습니다.\n",
            "28600개의 텍스트를 토큰화했습니다.\n",
            "28800개의 텍스트를 토큰화했습니다.\n",
            "29000개의 텍스트를 토큰화했습니다.\n",
            "29200개의 텍스트를 토큰화했습니다.\n",
            "29400개의 텍스트를 토큰화했습니다.\n",
            "29600개의 텍스트를 토큰화했습니다.\n",
            "29800개의 텍스트를 토큰화했습니다.\n",
            "30000개의 텍스트를 토큰화했습니다.\n",
            "30200개의 텍스트를 토큰화했습니다.\n",
            "30400개의 텍스트를 토큰화했습니다.\n",
            "30600개의 텍스트를 토큰화했습니다.\n",
            "30800개의 텍스트를 토큰화했습니다.\n",
            "31000개의 텍스트를 토큰화했습니다.\n",
            "31200개의 텍스트를 토큰화했습니다.\n",
            "31400개의 텍스트를 토큰화했습니다.\n",
            "31600개의 텍스트를 토큰화했습니다.\n",
            "31800개의 텍스트를 토큰화했습니다.\n",
            "32000개의 텍스트를 토큰화했습니다.\n",
            "200개의 텍스트를 토큰화했습니다.\n",
            "400개의 텍스트를 토큰화했습니다.\n",
            "600개의 텍스트를 토큰화했습니다.\n",
            "800개의 텍스트를 토큰화했습니다.\n",
            "1000개의 텍스트를 토큰화했습니다.\n",
            "1200개의 텍스트를 토큰화했습니다.\n",
            "1400개의 텍스트를 토큰화했습니다.\n",
            "1600개의 텍스트를 토큰화했습니다.\n",
            "1800개의 텍스트를 토큰화했습니다.\n",
            "2000개의 텍스트를 토큰화했습니다.\n",
            "2200개의 텍스트를 토큰화했습니다.\n",
            "2400개의 텍스트를 토큰화했습니다.\n",
            "2600개의 텍스트를 토큰화했습니다.\n",
            "2800개의 텍스트를 토큰화했습니다.\n",
            "3000개의 텍스트를 토큰화했습니다.\n",
            "3200개의 텍스트를 토큰화했습니다.\n",
            "3400개의 텍스트를 토큰화했습니다.\n",
            "3600개의 텍스트를 토큰화했습니다.\n",
            "3800개의 텍스트를 토큰화했습니다.\n",
            "4000개의 텍스트를 토큰화했습니다.\n",
            "4200개의 텍스트를 토큰화했습니다.\n",
            "4400개의 텍스트를 토큰화했습니다.\n",
            "4600개의 텍스트를 토큰화했습니다.\n",
            "4800개의 텍스트를 토큰화했습니다.\n",
            "5000개의 텍스트를 토큰화했습니다.\n",
            "5200개의 텍스트를 토큰화했습니다.\n",
            "5400개의 텍스트를 토큰화했습니다.\n",
            "5600개의 텍스트를 토큰화했습니다.\n",
            "5800개의 텍스트를 토큰화했습니다.\n",
            "6000개의 텍스트를 토큰화했습니다.\n",
            "6200개의 텍스트를 토큰화했습니다.\n",
            "6400개의 텍스트를 토큰화했습니다.\n",
            "6600개의 텍스트를 토큰화했습니다.\n",
            "6800개의 텍스트를 토큰화했습니다.\n",
            "7000개의 텍스트를 토큰화했습니다.\n",
            "7200개의 텍스트를 토큰화했습니다.\n",
            "7400개의 텍스트를 토큰화했습니다.\n",
            "7600개의 텍스트를 토큰화했습니다.\n",
            "7800개의 텍스트를 토큰화했습니다.\n",
            "8000개의 텍스트를 토큰화했습니다.\n",
            "토큰화 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 Attention + LSTM 모델\n"
      ],
      "metadata": {
        "id": "r18pg_gpHD8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1. 모델 설계"
      ],
      "metadata": {
        "id": "Tzm6yCCWBfqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2.1.1. Attention 매커니즘"
      ],
      "metadata": {
        "id": "9_Q62ntdVlVz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1GPAtwGVd_B"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Attention 가중치를 위한 weight 생성\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Attention score 계산\n",
        "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
        "        a = tf.keras.backend.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return tf.keras.backend.sum(output, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1.2. LSTM 모델 설계"
      ],
      "metadata": {
        "id": "XC_jqhRNZtS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 토큰화된 결과를 숫자 시퀀스로 변환\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train_tokenized)\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train_tokenized)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test_tokenized)\n",
        "\n",
        "# 패딩 처리\n",
        "MAX_LENGTH = max(len(s) for s in X_train_sequences)\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=MAX_LENGTH, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "# 변수 정의\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1  # 단어의 개수 + 1 (0을 고려하여)\n",
        "EMBEDDING_DIM = 128  # 임베딩 차원\n",
        "\n",
        "# 모델 구성\n",
        "input_layer = Input(shape=(MAX_LENGTH,))\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMBEDDING_DIM)(input_layer)\n",
        "lstm_layer = LSTM(128, return_sequences=True)(embedding_layer)\n",
        "attention_layer = AttentionLayer()(lstm_layer)\n",
        "output_layer = Dense(1, activation=\"sigmoid\")(attention_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUNq-U7OGw55",
        "outputId": "8581f126-dfca-4047-ad21-91a88ff9bfc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1908)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 1908, 128)         15771008  \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1908, 128)         131584    \n",
            "                                                                 \n",
            " attention_layer (AttentionL  (None, 128)              2036      \n",
            " ayer)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,904,757\n",
            "Trainable params: 15,904,757\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 학습하기"
      ],
      "metadata": {
        "id": "HpDSycz0zXb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "input_layer = Input(shape=(MAX_LENGTH,))\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMBEDDING_DIM)(input_layer)\n",
        "lstm_layer = LSTM(128, return_sequences=True)(embedding_layer)\n",
        "attention_layer = AttentionLayer()(lstm_layer)\n",
        "\n",
        "# Dropout 추가하여 적정 성능 유도\n",
        "dropout_layer = Dropout(0.5)(attention_layer)\n",
        "output_layer = Dense(1, activation=\"sigmoid\")(dropout_layer)\n",
        "\n",
        "model_attention_lstm = Model(inputs=input_layer, outputs=output_layer)\n",
        "model_attention_lstm.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "bVaq53X7ZNon"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping을 사용하여 성능 향상이 없을 때 학습을 조기 종료하는 기법을 적용하였습니다.\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# 모델 학습\n",
        "history_attention_lstm = model_attention_lstm.fit(\n",
        "    X_train_padded, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_padded, y_test),\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "ii_FLJuEbRJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e266e2c3-0b21-467f-b79c-68cd9872fbc4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1000/1000 [==============================] - 182s 174ms/step - loss: 0.0766 - accuracy: 0.9705 - val_loss: 0.0235 - val_accuracy: 0.9962\n",
            "Epoch 2/20\n",
            "1000/1000 [==============================] - 100s 100ms/step - loss: 0.0288 - accuracy: 0.9954 - val_loss: 0.0432 - val_accuracy: 0.9921\n",
            "Epoch 3/20\n",
            "1000/1000 [==============================] - 82s 82ms/step - loss: 0.0146 - accuracy: 0.9980 - val_loss: 0.0342 - val_accuracy: 0.9944\n",
            "Epoch 4/20\n",
            "1000/1000 [==============================] - 79s 79ms/step - loss: 0.0208 - accuracy: 0.9966 - val_loss: 0.0415 - val_accuracy: 0.9930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3. SAVE/LOAD"
      ],
      "metadata": {
        "id": "Zite23EIbBHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3.1. save"
      ],
      "metadata": {
        "id": "tWwSPV3sB-oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "save_path_attention_lstm = '/content/drive/MyDrive/finalmodel/attention_lstm_model.h5'\n",
        "model_attention_lstm.save(save_path_attention_lstm)\n",
        "\n",
        "print(f\"모델이 '{save_path_attention_lstm}' 경로에 저장되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPZ70fsSAXlu",
        "outputId": "834b722a-5b9b-47b3-87ff-48ba7adb0f58"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델이 '/content/drive/MyDrive/finalmodel/attention_lstm_model.h5' 경로에 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3.2. load"
      ],
      "metadata": {
        "id": "VEB9CpofbCzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# 저장한 모델 불러오기\n",
        "model_attention_lstm = load_model(save_path_attention_lstm, custom_objects={\"AttentionLayer\": AttentionLayer})\n",
        "\n",
        "# 모델 요약 출력\n",
        "model_attention_lstm.summary()\n",
        "\n",
        "print(f\"모델이 '{save_path_attention_lstm}' 경로에서 불러와졌습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx-y_rcka-8t",
        "outputId": "7beab980-97ba-426f-a917-ef4aecb2850f"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 1908)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 1908, 128)         15771008  \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1908, 128)         131584    \n",
            "                                                                 \n",
            " attention_layer_1 (Attentio  (None, 128)              2036      \n",
            " nLayer)                                                         \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,904,757\n",
            "Trainable params: 15,904,757\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "모델이 '/content/drive/MyDrive/finalmodel/attention_lstm_model.h5' 경로에서 불러와졌습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4. 성능 평가"
      ],
      "metadata": {
        "id": "hzlaC5RrcfJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model_attention_lstm.evaluate(X_test_padded, y_test, batch_size=32)\n",
        "\n",
        "print(f\"Attention + LSTM 모델의 테스트 손실: {loss:.4f}\")\n",
        "print(f\"Attention + LSTM 모델의 테스트 정확도: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao0Em4IN1C5j",
        "outputId": "45030767-2b65-4c13-b2a7-f0d606595313"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250/250 [==============================] - 6s 26ms/step - loss: 0.0415 - accuracy: 0.9930\n",
            "Attention + LSTM 모델의 테스트 손실: 0.0415\n",
            "Attention + LSTM 모델의 테스트 정확도: 0.9930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.  KoBERT 모델"
      ],
      "metadata": {
        "id": "mhE_m4WPZ5aX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.모델 구축"
      ],
      "metadata": {
        "id": "m5Qg33nPYFyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Vm2rMcbiT9cn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cefd242f-fbe6-4736-9f2b-d351a47b110c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 학습률 조정\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "# 모델 구성 및 컴파일\n",
        "model_kobert.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_kobert.summary()"
      ],
      "metadata": {
        "id": "Wb-zSnqaYEo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef2c780-c625-435a-ab43-0f8cb6f01698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bert_for_sequence_classification_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  92186880  \n",
            "                                                                 \n",
            " dropout_190 (Dropout)       multiple                  0         \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 92,188,418\n",
            "Trainable params: 92,188,418\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2. 학습하기"
      ],
      "metadata": {
        "id": "FUlE3wBzbePQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "tHpCEIIEak8w"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBERT 토크나이저와 모델 로드\n",
        "tokenizer_kobert = BertTokenizer.from_pretrained('monologg/kobert')\n",
        "model_kobert = TFBertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2) # num_labels는 분류할 레이블의 수에 따라 조정하세요.\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "MAX_LENGTH = 128  # 원하는 문장 최대 길이 설정\n",
        "\n",
        "# KoBERT를 사용할 때, 입력 데이터는 토큰화 후 패딩 처리된 것이 아닌 원래의 텍스트 데이터를 사용해야 합니다.\n",
        "train_encodings = tokenizer_kobert(list(X_train), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n",
        "test_encodings = tokenizer_kobert(list(X_test), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n",
        "\n",
        "# Early Stopping을 사용하여 성능 향상이 없을 때 학습을 조기 종료하는 기법을 적용하였습니다.\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = ['accuracy']\n",
        "\n",
        "model_kobert.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "metadata": {
        "id": "SnujzzvWCKnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "history_kobert = model_kobert.fit(\n",
        "    [train_encodings['input_ids'], train_encodings['attention_mask']], y_train,\n",
        "    epochs=12,\n",
        "    batch_size=32,\n",
        "    validation_data=([test_encodings['input_ids'], test_encodings['attention_mask']], y_test),\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "tw2ruJwzZ5PF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c0d994-56ee-4ba5-9477-098473a754ef"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "1000/1000 [==============================] - 2574s 855ms/step - loss: 0.0516 - accuracy: 0.9870 - val_loss: 0.1398 - val_accuracy: 0.9649\n",
            "Epoch 2/12\n",
            "1000/1000 [==============================] - 864s 865ms/step - loss: 0.1291 - accuracy: 0.9671 - val_loss: 0.1258 - val_accuracy: 0.9668\n",
            "Epoch 3/12\n",
            "1000/1000 [==============================] - 863s 863ms/step - loss: 0.1174 - accuracy: 0.9698 - val_loss: 0.1271 - val_accuracy: 0.9664\n",
            "Epoch 4/12\n",
            "1000/1000 [==============================] - 863s 863ms/step - loss: 0.1346 - accuracy: 0.9660 - val_loss: 0.1636 - val_accuracy: 0.9594\n",
            "Epoch 5/12\n",
            "1000/1000 [==============================] - 862s 862ms/step - loss: 0.1197 - accuracy: 0.9683 - val_loss: 0.1332 - val_accuracy: 0.9661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3. SAVE/LOAD"
      ],
      "metadata": {
        "id": "PnpjXl0IbLVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "save"
      ],
      "metadata": {
        "id": "lecJfUvuCIv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장하기\n",
        "save_path = ('/content/drive/MyDrive/finalmodel/model_kobert')\n",
        "model_kobert.save_pretrained(save_path)\n",
        "tokenizer_kobert.save_pretrained(save_path)\n",
        "\n",
        "print(f\"모델과 토크나이저가 '{save_path}' 경로에 저장되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovvEjO6zbLVp",
        "outputId": "0a0bc380-b1e8-4eae-ab12-e9dc82f7aa3a"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델과 토크나이저가 '/content/drive/MyDrive/finalmodel/model_kobert' 경로에 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "load"
      ],
      "metadata": {
        "id": "5Mwl5eRIbLVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "\n",
        "# 저장한 모델과 토크나이저 불러오기\n",
        "loaded_tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/finalmodel/model_kobert')\n",
        "loaded_model = TFBertForSequenceClassification.from_pretrained('/content/drive/MyDrive/finalmodel/model_kobert')\n",
        "\n",
        "print(\"모델과 토크나이저가 불러와졌습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA1LaEqabLVp",
        "outputId": "a8fb85ec-97ca-4cd9-eeda-56869bf8ba77"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at /content/drive/MyDrive/finalmodel/model_kobert were not used when initializing TFBertForSequenceClassification: ['dropout_228']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/finalmodel/model_kobert.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델과 토크나이저가 불러와졌습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 초기화 (필요 시)"
      ],
      "metadata": {
        "id": "fNJGOUSl7mGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "\n",
        "# 원본 모델과 토크나이저 로드\n",
        "original_tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
        "original_model = TFBertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2)\n",
        "\n",
        "# 로드한 모델의 가중치를 원본 모델의 가중치로 복사\n",
        "loaded_model.set_weights(original_model.get_weights())\n",
        "loaded_tokenizer = original_tokenizer\n",
        "\n",
        "print(\"모델과 토크나이저가 원본 모델에서 초기화되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39Q31wEsv82E",
        "outputId": "2ede945f-c68c-434b-8461-667c1fe6c742"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델과 토크나이저가 원본 모델에서 초기화되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.4. 평가하기"
      ],
      "metadata": {
        "id": "-ZQROZQyqR7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 모델 컴파일\n",
        "loaded_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 테스트 데이터의 인코딩\n",
        "test_encodings = loaded_tokenizer(list(X_test), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n",
        "\n",
        "# 모델 성능 평가\n",
        "loss, accuracy = model_kobert.evaluate(\n",
        "    [test_encodings['input_ids'], test_encodings['attention_mask']], y_test\n",
        ")\n",
        "\n",
        "print(f\"테스트 데이터에 대한 손실: {loss:.4f}\")\n",
        "print(f\"테스트 데이터에 대한 정확도: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLdA3HxBzBXU",
        "outputId": "1d4e5794-e47f-4143-e27c-2ec32a0ebf78"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250/250 [==============================] - 60s 241ms/step - loss: 0.1332 - accuracy: 0.9661\n",
            "테스트 데이터에 대한 손실: 0.1332\n",
            "테스트 데이터에 대한 정확도: 0.9661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 앙상블"
      ],
      "metadata": {
        "id": "EgONcodDn2zY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 소프트 보팅 방법 채택"
      ],
      "metadata": {
        "id": "tE0cWVkDHJFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "# 모델 컴파일\n",
        "loaded_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 테스트 데이터의 인코딩\n",
        "test_encodings = loaded_tokenizer(list(X_test), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n",
        "\n",
        "# 각 모델의 예측 결과 계산\n",
        "prob_kobert = loaded_model.predict([test_encodings['input_ids'], test_encodings['attention_mask']]).logits\n",
        "prob_attention_lstm = model_attention_lstm.predict(X_test_padded)\n",
        "\n",
        "# 로짓 값을 확률로 변환\n",
        "probabilities_kobert = tf.nn.sigmoid(prob_kobert)\n",
        "probabilities_attention_lstm = tf.nn.sigmoid(prob_attention_lstm)\n",
        "\n",
        "# 앙상블을 위한 예측 확률 계산\n",
        "ensemble_probabilities = (probabilities_kobert + probabilities_attention_lstm) / 2\n",
        "\n",
        "# 앙상블 결과를 소프트 보팅하여 예측 클래스 계산\n",
        "ensemble_predicted_classes = np.argmax(ensemble_probabilities, axis=1)\n",
        "\n",
        "# 정확도 평가\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_predicted_classes)\n",
        "print(f\"모델 앙상블의 예측 정확도: {ensemble_accuracy:.4f}\")\n",
        "\n",
        "# 로스 평가\n",
        "ensemble_loss = log_loss(y_test, ensemble_probabilities)\n",
        "print(f\"모델 앙상블의 로스: {ensemble_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWjYnHsjrctu",
        "outputId": "0d2fc3b9-2ad2-4551-c9a1-3f735d959147"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250/250 [==============================] - 62s 239ms/step\n",
            "250/250 [==============================] - 6s 23ms/step\n",
            "모델 앙상블의 예측 정확도: 0.9661\n",
            "모델 앙상블의 로스: 0.4405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 최종 평가"
      ],
      "metadata": {
        "id": "XtR9wtstn-cX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1. 모델 평가"
      ],
      "metadata": {
        "id": "xA3VC_1xD6UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 정확도 평가\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_predicted_classes)\n",
        "print(f\"앙상블 모델의 예측 정확도: {ensemble_accuracy:.4f}\")\n",
        "\n",
        "# 정밀도 평가\n",
        "precision = precision_score(y_test, ensemble_predicted_classes)\n",
        "print(f\"앙상블 모델의 정밀도: {precision:.4f}\")\n",
        "\n",
        "# 재현율 평가\n",
        "recall = recall_score(y_test, ensemble_predicted_classes)\n",
        "print(f\"앙상블 모델의 재현율: {recall:.4f}\")\n",
        "\n",
        "# F1 점수 평가\n",
        "f1 = f1_score(y_test, ensemble_predicted_classes)\n",
        "print(f\"앙상블 모델의 F1 점수: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "mz660PsDn-FC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fae930-f792-4029-afc0-65df28bd708a"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "앙상블 모델의 예측 정확도: 0.9661\n",
            "앙상블 모델의 정밀도: 0.9992\n",
            "앙상블 모델의 재현율: 0.9331\n",
            "앙상블 모델의 F1 점수: 0.9650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.2. I/O **테스트**"
      ],
      "metadata": {
        "id": "DGZ8AyXPCfgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 불러온 모델 및 토크나이저 설정\n",
        "loaded_tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/finalmodel/model_kobert')\n",
        "loaded_model = TFBertForSequenceClassification.from_pretrained('/content/drive/MyDrive/finalmodel/model_kobert')\n",
        "\n",
        "# 텍스트 입력 받기\n",
        "text = input(\"예측할 텍스트를 입력하세요: \")\n",
        "\n",
        "# 입력 텍스트의 전처리 및 인코딩\n",
        "encoded_input = loaded_tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
        "\n",
        "# 각 모델의 예측 결과 계산\n",
        "prob_kobert = loaded_model.predict([encoded_input['input_ids'], encoded_input['attention_mask']]).logits\n",
        "prob_attention_lstm = model_attention_lstm.predict(X_test_padded)\n",
        "\n",
        "# 로짓 값을 확률로 변환\n",
        "probabilities_kobert = tf.nn.sigmoid(prob_kobert)\n",
        "probabilities_attention_lstm = tf.nn.sigmoid(prob_attention_lstm)\n",
        "\n",
        "# 앙상블을 위한 예측 확률 계산\n",
        "ensemble_probabilities = (probabilities_kobert + probabilities_attention_lstm) / 2\n",
        "\n",
        "# 앙상블 결과를 소프트 보팅하여 예측 클래스 계산\n",
        "ensemble_predicted_class = np.argmax(ensemble_probabilities, axis=1)[0]\n",
        "\n",
        "# 예측 클래스와 확률 출력\n",
        "class_names = ['정상', '스팸']  # 클래스 이름 설정\n",
        "predicted_class_name = class_names[ensemble_predicted_class]\n",
        "predicted_class_probability = ensemble_probabilities[0][ensemble_predicted_class]\n",
        "print(f\"예측: {predicted_class_name}\")\n",
        "print(f\"확률: {predicted_class_probability:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_ss1gCm3Lmf",
        "outputId": "1bd89482-61d7-4cae-e2cb-3e548196f581"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at /content/drive/MyDrive/finalmodel/model_kobert were not used when initializing TFBertForSequenceClassification: ['dropout_228']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/finalmodel/model_kobert.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측할 텍스트를 입력하세요: 앙상블 결과를 소프트 보팅하여 예측 클래스 계산\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "250/250 [==============================] - 6s 23ms/step\n",
            "예측: 정상\n",
            "확률: 0.6318\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}