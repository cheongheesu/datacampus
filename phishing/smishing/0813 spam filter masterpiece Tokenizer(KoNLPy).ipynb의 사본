{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ah3tzjOMLlK2"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ND-nuPx_MU9e"},"outputs":[],"source":["# df_97이라는 데이터가 있다고 가정\n","df_97 = pd.read_csv(\"/content/drive/MyDrive/spam_data_free/combine_spam_labeled.csv\")"]},{"cell_type":"code","source":["df_97.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pCPud_MKvzm9","executionInfo":{"status":"ok","timestamp":1691987305714,"user_tz":-540,"elapsed":11,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"6900d56f-ab50-4b98-e120-e260c141b9ce"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(494615, 2)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOHg6i0xLiKf"},"outputs":[],"source":["# 데이터를 학습 데이터와 테스트 데이터로 나눕니다.\n","X_train, X_test, y_train, y_test = train_test_split(df_97['utterances'], df_97['label'], test_size=0.2, random_state=42)"]},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53xSkbgPQ-Dr","executionInfo":{"status":"ok","timestamp":1691987310882,"user_tz":-540,"elapsed":4566,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"adda11fb-3b2c-4204-cc76-3ef73aa07059"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PljpU-1WLnOy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e8ed6c6c-1b78-43dd-8e15-0e2614cd79fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["200개의 텍스트를 토큰화했습니다.\n","400개의 텍스트를 토큰화했습니다.\n","600개의 텍스트를 토큰화했습니다.\n","800개의 텍스트를 토큰화했습니다.\n","1000개의 텍스트를 토큰화했습니다.\n","1200개의 텍스트를 토큰화했습니다.\n","1400개의 텍스트를 토큰화했습니다.\n","1600개의 텍스트를 토큰화했습니다.\n","1800개의 텍스트를 토큰화했습니다.\n","2000개의 텍스트를 토큰화했습니다.\n","2200개의 텍스트를 토큰화했습니다.\n","2400개의 텍스트를 토큰화했습니다.\n","2600개의 텍스트를 토큰화했습니다.\n","2800개의 텍스트를 토큰화했습니다.\n","3000개의 텍스트를 토큰화했습니다.\n","3200개의 텍스트를 토큰화했습니다.\n","3400개의 텍스트를 토큰화했습니다.\n","3600개의 텍스트를 토큰화했습니다.\n","3800개의 텍스트를 토큰화했습니다.\n","4000개의 텍스트를 토큰화했습니다.\n","4200개의 텍스트를 토큰화했습니다.\n","4400개의 텍스트를 토큰화했습니다.\n","4600개의 텍스트를 토큰화했습니다.\n","4800개의 텍스트를 토큰화했습니다.\n","5000개의 텍스트를 토큰화했습니다.\n","5200개의 텍스트를 토큰화했습니다.\n","5400개의 텍스트를 토큰화했습니다.\n","5600개의 텍스트를 토큰화했습니다.\n","5800개의 텍스트를 토큰화했습니다.\n","6000개의 텍스트를 토큰화했습니다.\n","6200개의 텍스트를 토큰화했습니다.\n","6400개의 텍스트를 토큰화했습니다.\n","6600개의 텍스트를 토큰화했습니다.\n","6800개의 텍스트를 토큰화했습니다.\n","7000개의 텍스트를 토큰화했습니다.\n"]}],"source":["import os\n","import json\n","\n","from konlpy.tag import Okt\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Okt 객체 생성\n","okt = Okt()\n","\n","# 토큰화 함수 정의\n","def tokenize(texts, save_path=\"/content/drive/MyDrive/spam_data_free/combine_spam_labeling.json\"):\n","    tokenized_texts = []\n","\n","    # 이전에 저장된 데이터가 있다면 불러옴\n","    if os.path.exists(save_path):\n","        with open(save_path, 'r', encoding='utf-8') as f:\n","            tokenized_texts = json.load(f)\n","\n","    start_idx = len(tokenized_texts)\n","    for idx, text in enumerate(texts[start_idx:]):\n","        absolute_idx = start_idx + idx\n","        if (absolute_idx + 1) % 200 == 0:\n","            print(f\"{absolute_idx + 1}개의 텍스트를 토큰화했습니다.\")\n","\n","            # 토큰화된 데이터를 파일에 저장\n","            with open(save_path, 'w', encoding='utf-8') as f:\n","                json.dump(tokenized_texts, f)\n","\n","        tokenized_texts.append(okt.morphs(text))\n","    return tokenized_texts\n","\n","X_train_tokenized = tokenize(X_train)\n","X_test_tokenized = tokenize(X_test)\n","\n","# Keras의 Tokenizer로 텍스트 데이터를 숫자로 변환\n","tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(X_train_tokenized)\n","\n","X_train_sequences = tokenizer.texts_to_sequences(X_train_tokenized)\n","X_test_sequences = tokenizer.texts_to_sequences(X_test_tokenized)\n","\n","# 패딩 처리\n","X_train_padded = pad_sequences(X_train_sequences, maxlen=100, padding='post', truncating='post')\n","X_test_padded = pad_sequences(X_test_sequences, maxlen=100, padding='post', truncating='post')\n"]},{"cell_type":"code","source":["import pickle\n","\n","save_path = \"/content/drive/MyDrive/spam_data_free/\"\n","\n","# 토큰화 결과 저장\n","with open(save_path + \"X_train_tokenized.pkl\", \"wb\") as f:\n","    pickle.dump(X_train_tokenized, f)\n","\n","with open(save_path + \"X_test_tokenized.pkl\", \"wb\") as f:\n","    pickle.dump(X_test_tokenized, f)\n","\n","# Keras Tokenizer 객체 저장\n","with open(save_path + \"tokenizer.pkl\", \"wb\") as f:\n","    pickle.dump(tokenizer, f)\n","\n","# 패딩 처리한 결과 저장\n","with open(save_path + \"X_train_padded.pkl\", \"wb\") as f:\n","    pickle.dump(X_train_padded, f)\n","\n","with open(save_path + \"X_test_padded.pkl\", \"wb\") as f:\n","    pickle.dump(X_test_padded, f)\n"],"metadata":{"id":"flDmxjKxPgfg"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1cY7fqqzUghF1e4k4duNKuPKLg557SI45","authorship_tag":"ABX9TyPLJZ23OC0uOXavPrILyfXt"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}